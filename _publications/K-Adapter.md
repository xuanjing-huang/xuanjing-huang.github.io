---
title: "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"
collection: publications
permalink: /publication/K-Adapter
excerpt: 'We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically
update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected,
the historically injected knowledge would be flushed away. To address this, we propose KADAPTER, a framework that retains the original parameters of the pre-trained model fixed and supports the development of versatile knowledge-infused model. Taking RoBERTa as the backbone model, K-ADAPTER has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus multiple adapters can be efficiently trained in a distributed way. '
date: 2021-08-01
venue: 'Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021'
paperurl: 'http://academicpages.github.io/files/K-Adapter.pdf'
citation: 'Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ACL/IJCNLP (Findings) 2021: 1405-1418'
---
[[BibTex]](https://aclanthology.org/2021.findings-acl.121.bib)[[PDF]](https://aclanthology.org/2021.findings-acl.121.pdf)

[Download paper here](http://academicpages.github.io/files/K-Adapter.pdf)

Recommended citation: Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ACL/IJCNLP (Findings) 2021: 1405-1418.
