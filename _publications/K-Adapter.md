---
title: "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"
collection: publications
permalink: /publication/K-Adapter
excerpt: 'A framework that retains the original parameters of the pre-trained model fixed and supports the development of versatile knowledge-infused model.'
date: 2021-08-01
venue: 'Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021'
paperurl: 'http://academicpages.github.io/files/K-Adapter.pdf'
citation: 'Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ACL/IJCNLP (Findings) 2021: 1405-1418'
---
[[BibTex]](https://aclanthology.org/2021.findings-acl.121.bib)[[PDF]](https://aclanthology.org/2021.findings-acl.121.pdf)

[Download paper here](http://academicpages.github.io/files/K-Adapter.pdf)

Recommended citation: Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ACL/IJCNLP (Findings) 2021: 1405-1418.
